<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://fengzhiheng.github.io').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="用的什么教科书？Elementary Linear Algebra - A Matrix Approach, 2nd Ed., by L. E. Spence, A. J. Insel and S. H. Friedberg">
<meta property="og:type" content="article">
<meta property="og:title" content="LinearAlgebra笔记">
<meta property="og:url" content="https://fengzhiheng.github.io/2020/02/06/LinearAlgebra%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Feng Zhiheng&#39;s Blog">
<meta property="og:description" content="用的什么教科书？Elementary Linear Algebra - A Matrix Approach, 2nd Ed., by L. E. Spence, A. J. Insel and S. H. Friedberg">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-02-06T03:46:24.000Z">
<meta property="article:modified_time" content="2020-02-19T07:29:30.423Z">
<meta property="article:author" content="Feng Zhiheng">
<meta property="article:tag" content="未整理">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://fengzhiheng.github.io/2020/02/06/LinearAlgebra%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>LinearAlgebra笔记 | Feng Zhiheng's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Feng Zhiheng's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fengzhiheng.github.io/2020/02/06/LinearAlgebra%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Feng Zhiheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Feng Zhiheng's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LinearAlgebra笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-02-06 11:46:24" itemprop="dateCreated datePublished" datetime="2020-02-06T11:46:24+08:00">2020-02-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-19 15:29:30" itemprop="dateModified" datetime="2020-02-19T15:29:30+08:00">2020-02-19</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>33k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>56 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>用的什么教科书？<br>Elementary Linear Algebra - A Matrix Approach, 2nd Ed., by L. E. Spence, A. J. Insel and S. H. Friedberg</p>
<a id="more"></a> 
<hr>
<ol>
<li><p>为什么要学习线性代数？<br> 线性系统有很多application：<br> 信号系统、计算机graphics、Google PageRank（2013年之后就不再update了）</p>
</li>
<li><p>线性代数中最重要，最原始的基本概念有哪些？<br>chapter1、chapter2、chapter3：</p>
<ul>
<li>讨论的是，有一个线性系统，给你输出，问你，什么样的输入能够得到这样的输出？到底有没有solution？这个solution是唯一的吗？我们怎么找到这个解？</li>
</ul>
<p>chapter 4</p>
<ul>
<li>输入是一个集合，集合中的每一个元素给你，你都会得到一个输出，第四章问的问题是，你怎么描述这样的一个“集合”？——当然用的不是集合这个概念进行描述的，会用另外的词汇，描述线性系统输出的所有总和</li>
<li>维度的概念</li>
<li>描述这个集合的方法有很多种，你可选择比较容易的方法来描述这个集合，你的线性系统就会改变，可能会变得简单的线性系统。</li>
</ul>
<p>chapter 7</p>
<ul>
<li>假如，我们指定的输出不在我们输出的集合中。但是我们怎么在可能的输出中，找到一个和指定输出最近的呢？</li>
</ul>
<p>chapter 5</p>
<ul>
<li>eigenXXX </li>
<li>有些线性系统，它会有些特殊的输入，它们的输出是乘上了一个scalar</li>
</ul>
</li>
<li><p>线性代数讨论的是什么问题？</p>
<ul>
<li>线性系统</li>
</ul>
</li>
<li><p>现实生活中如何利用和使用线性代数来分析和解决问题？</p>
<hr>
<h3 id="线性代数这门课讨论的是什么？"><a href="#线性代数这门课讨论的是什么？" class="headerlink" title="线性代数这门课讨论的是什么？"></a>线性代数这门课讨论的是什么？</h3><p>线性代数这门课讨论的就是线性系统；</p>
</li>
</ol>
<h3 id="什么是线性系统？"><a href="#什么是线性系统？" class="headerlink" title="什么是线性系统？"></a>什么是线性系统？</h3><p>线性系统有两个基本的特点：<br>一个是可加性；<br>一个是可乘性；  </p>
<p>叠加性(preserving  addiction)和齐次性（preserving multiplication）</p>
<p>在这门课中，我们会针对线性系统做哪些了解和分析？</p>
<ol>
<li>给你一个线性系统，然后还有这个线性系统的输出，接下来问你，怎么样的输入会有这样的输出？</li>
</ol>
<h3 id="线性系统会关心什么问题？"><a href="#线性系统会关心什么问题？" class="headerlink" title="线性系统会关心什么问题？"></a>线性系统会关心什么问题？</h3><p>接下来就有不同的问题，需要讨论（前3章讨论的问题）<br>1）到底有没有输出？<br>2）如果找到某种输入可以得到这样的输出，那么它是唯一的吗？<br>3）第三个问题是，怎么找到这个解？<br>4）最后，讲一下行列式 </p>
<p>从第四章开始，我们讨论的问题是？更General？</p>
<p>输入是一个集合（set），输出也是一个set，那么如何描述一个set呢？（其实并不是用集合的词汇）<br>维度的概念，维度到底指的是什么？<br>维度的描述，有很多种，你可以选择一种简单的方式，当你选择之后，你的线性系统就会发生改变。</p>
<p>第七章，假设我们想要的输出，不在指定的输出集合中，那我们会找个最接近的</p>
<p>第五章，讲eigenXXX，<br>滤波器，某种类型的信号被放大，或被缩小。</p>
<p>结论：要学好线性代数。</p>
<p>Linear System = A system of linear equations（多元一次联立方程）</p>
<p>m个equation，n个variables</p>
<h3 id="线性代数中基本概念？"><a href="#线性代数中基本概念？" class="headerlink" title="线性代数中基本概念？"></a>线性代数中基本概念？</h3><p>function f、Domain(定义域)、Co-Domain（对应域）、Range（值域）、值域是对应域的子集、one-to-one（一对一）、Onto（映成）、Co-domain= range</p>
<p>微分是线性的Derivative<br><a href="https://youtu.be/ZexDYHpmID8?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=840" target="_blank" rel="noopener">https://youtu.be/ZexDYHpmID8?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=840</a></p>
<p>积分也是线性的 Integral<br><a href="https://youtu.be/ZexDYHpmID8?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1011" target="_blank" rel="noopener">https://youtu.be/ZexDYHpmID8?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1011</a></p>
<p>连一个function都可能是一个向量</p>
<p>A Linear System is described by a system of linear equation</p>
<p><a href="https://youtu.be/tpNFMU7KsEU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=851" target="_blank" rel="noopener">https://youtu.be/tpNFMU7KsEU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=851</a><br>两个人因为相爱在一起，和两个人在一起会相爱，是不一样的意思。</p>
<p>符合这8个特质的就叫做向量；<br>什么是matrix，可以想象成一组vector的集合  </p>
<p><a href="https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=86" target="_blank" rel="noopener">https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=86</a><br>今天讨论的问题是，线性系统是不是有解？</p>
<p>linear combination???<br>span???</p>
<h3 id="如何理解矩阵和向量相乘"><a href="#如何理解矩阵和向量相乘" class="headerlink" title="如何理解矩阵和向量相乘"></a>如何理解矩阵和向量相乘</h3><p>矩阵和向量相乘，其实就是对矩阵的列向量做linear combination </p>
<p>Ax = b 是不是有解，等价于，<br>Non empty solution set<br>Has soulution or not?<br>consistent?<br>同时，等价于，b是不是A的column的line<br>linear combination</p>
<p><a href="https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=737" target="_blank" rel="noopener">https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=737</a><br>从linear combination的角度，分析这个线性系统是不是有解？</p>
<p><a href="https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1064" target="_blank" rel="noopener">https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1064</a><br>只要u和v不是平行的（2维向量）， 且不是0向量，那么一定是有解得</p>
<p>但是对于3维向量，3个向量都不是平行的，仍然不能保证能够扫过整个R3空间。为什么？请看<a href="https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1188" target="_blank" rel="noopener">https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1188</a><br>这里涉及一个independent的概念，如果，3个向量是independent的，才能扫过整个R3空间</p>
<p>有解和平行，并不是充要条件。<br>u和v非平行，一定有解。<br>有解的话，u和v也可能是平行的<br><a href="https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1382" target="_blank" rel="noopener">https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1382</a></p>
<h3 id="什么是Span"><a href="#什么是Span" class="headerlink" title="什么是Span"></a>什么是Span</h3><p>一个Vector set中所有向量的linear combination（穷举所有的coefficient）构成的向量的空间几何就是span</p>
<p><a href="https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1945" target="_blank" rel="noopener">https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1945</a><br>什么是stand vector？</p>
<h3 id="有没有解的陈述，可是换一种说法"><a href="#有没有解的陈述，可是换一种说法" class="headerlink" title="有没有解的陈述，可是换一种说法"></a>有没有解的陈述，可是换一种说法</h3><ol>
<li>Has solution or not?</li>
<li>Is b the linear combination of column of A </li>
<li>Is b in the span of the columns of A<br><a href="https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2023" target="_blank" rel="noopener">https://youtu.be/-E67rZSjTNI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2023</a></li>
</ol>
<p><a href="https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=665" target="_blank" rel="noopener">https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=665</a><br>dependent 和 independent 的概念</p>
<p><a href="https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1256" target="_blank" rel="noopener">https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1256</a><br>只要有zero vector的话，这个vector set一定是dependent。</p>
<h3 id="dependent和有解没有解是没有关系的。"><a href="#dependent和有解没有解是没有关系的。" class="headerlink" title="dependent和有解没有解是没有关系的。"></a>dependent和有解没有解是没有关系的。</h3><p>dependent的影响是，如果你的System of linear equations有解，你就有无穷多个解。<br><a href="https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1536" target="_blank" rel="noopener">https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1536</a></p>
<h3 id="什么是homogeneous"><a href="#什么是homogeneous" class="headerlink" title="什么是homogeneous?"></a>什么是homogeneous?</h3><p><a href="https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1832" target="_blank" rel="noopener">https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1832</a><br>只要constant term是 zero vector，那么就说这个linear equations是homogeneous</p>
<h3 id="Ax-0有非零的解，那么A就是dependent，为什么？因为dependent就是这样定义的呢。"><a href="#Ax-0有非零的解，那么A就是dependent，为什么？因为dependent就是这样定义的呢。" class="headerlink" title="Ax = 0有非零的解，那么A就是dependent，为什么？因为dependent就是这样定义的呢。"></a>Ax = 0有非零的解，那么A就是dependent，为什么？因为dependent就是这样定义的呢。</h3><p><a href="https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2098" target="_blank" rel="noopener">https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2098</a></p>
<p><a href="https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=81" target="_blank" rel="noopener">https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=81</a></p>
<h3 id="在有解的前提下，有多少个解呢？"><a href="#在有解的前提下，有多少个解呢？" class="headerlink" title="在有解的前提下，有多少个解呢？"></a>在有解的前提下，有多少个解呢？</h3><p>How many solutions?<br>the columns of A are independent<br>或者换一种说法：Rank A = n<br>或者换一种说法：Nullity A = 0<br>唯一解。</p>
<p>另外一种case，A的columns是dependent，<br>Rank A &lt; n;<br>Nullity &gt; 0;<br>无穷多个解。</p>
<p>只有两种可能，有唯一的一个解，或者无穷多个解？如何理解？    <a href="https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=266" target="_blank" rel="noopener">https://youtu.be/34HlThINCsc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=266</a></p>
<p>定义rank的时候，需要用到dependent和independent</p>
<p>什么是Rank，什么是Nullity?<br><a href="https://youtu.be/34HlThINCsc?t=2489" target="_blank" rel="noopener">https://youtu.be/34HlThINCsc?t=2489</a></p>
<h3 id="什么是一个Matrix的Rank呢？"><a href="#什么是一个Matrix的Rank呢？" class="headerlink" title="什么是一个Matrix的Rank呢？"></a>什么是一个Matrix的Rank呢？</h3><p>你可以找到最多的independent的column的数目；</p>
<p><a href="https://youtu.be/34HlThINCsc?t=2555" target="_blank" rel="noopener">https://youtu.be/34HlThINCsc?t=2555</a><br>Nullity的定义是什么呢？<br>Nullity = number of columns - rank</p>
<p>Dependent(Linear dependent 线性相关) 和Independent的定义是什么啊？<br>矩阵A的column vector的线性组合可以得到zero vector。而且coefficient not all zero。<br>有一组标量（scalar）不全为0，标量乘上对应的向量，得到zero Vector.</p>
<p><a href="https://en.wikipedia.org/wiki/Linear_independence" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Linear_independence</a></p>
<p>今天要学习的内容是，如何解一个linear system of equations<br><a href="https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=322" target="_blank" rel="noopener">https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=322</a></p>
<h3 id="equivalent（等价的）什么是等价。"><a href="#equivalent（等价的）什么是等价。" class="headerlink" title="equivalent（等价的）什么是等价。"></a>equivalent（等价的）什么是等价。</h3><p>如果两个system of linear equations有同样的解，那么说，这两个system of linear equations是equivalent</p>
<p>以下的操作不影响solution</p>
<ol>
<li>interchange(交换)</li>
<li>scaling（缩放）</li>
<li>row addition(行相加)</li>
</ol>
<h3 id="augmented-matrix增广矩阵"><a href="#augmented-matrix增广矩阵" class="headerlink" title="augmented matrix增广矩阵"></a>augmented matrix增广矩阵</h3><p>什么是增广矩阵<br><a href="https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=653" target="_blank" rel="noopener">https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=653</a></p>
<h3 id="什么是elementary-row-operation-（行变换？）"><a href="#什么是elementary-row-operation-（行变换？）" class="headerlink" title="什么是elementary row operation?（行变换？）"></a>什么是elementary row operation?（行变换？）</h3><p><a href="https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=915" target="_blank" rel="noopener">https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=915</a></p>
<h3 id="reduced-row-echelon-form-RREF-简化行梯形形式"><a href="#reduced-row-echelon-form-RREF-简化行梯形形式" class="headerlink" title="reduced row echelon form(RREF)(简化行梯形形式)"></a>reduced row echelon form(RREF)(简化行梯形形式)</h3><p>RREF</p>
<h3 id="什么是row-echelon-form（要满足两个条件）"><a href="#什么是row-echelon-form（要满足两个条件）" class="headerlink" title="什么是row echelon form（要满足两个条件）"></a>什么是row echelon form（要满足两个条件）</h3><ol>
<li>所有nonzero row都在zero row上边</li>
<li>the leading entries（每一行中，第一个不为零的数） are in echelon form<br><a href="https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=989" target="_blank" rel="noopener">https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=989</a></li>
</ol>
<h3 id="什么是Reduced-row-echelon-form（要满足三个条件）"><a href="#什么是Reduced-row-echelon-form（要满足三个条件）" class="headerlink" title="什么是Reduced row echelon form（要满足三个条件）"></a>什么是Reduced row echelon form（要满足三个条件）</h3><p>1-2. 和row echelon form一样<br>3. the columns containing the leading entries are standard vector（什么是standard vector，类似这种[0 0 1 0 0]）.<br><a href="https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1265" target="_blank" rel="noopener">https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1265</a></p>
<p>reduced row echelon form是unique的<br><a href="https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1465" target="_blank" rel="noopener">https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1465</a></p>
<h3 id="接下来要补充的一个概念是Pivot-column"><a href="#接下来要补充的一个概念是Pivot-column" class="headerlink" title="接下来要补充的一个概念是Pivot column"></a>接下来要补充的一个概念是Pivot column</h3><p>什么是pivot positions<br><a href="https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1640" target="_blank" rel="noopener">https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1640</a><br>有pivot position的column就是pivot column<br><a href="https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1672" target="_blank" rel="noopener">https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1672</a></p>
<p>什么是basic variables，什么是free variables?<br><a href="https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1942" target="_blank" rel="noopener">https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1942</a></p>
<p>parametric representations 的定义<br><a href="https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2017" target="_blank" rel="noopener">https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2017</a></p>
<p>练习高斯消去法没有什么意义，只是为了让你心情平静些？<br><a href="https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2235" target="_blank" rel="noopener">https://youtu.be/zuTH1WdREkY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2235</a></p>
<p>今天这节课的内容是，我们能从reduced row echelon form学到什么？<br><a href="https://youtu.be/ObibwhRY8xc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=26" target="_blank" rel="noopener">https://youtu.be/ObibwhRY8xc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=26</a></p>
<h3 id="Reduced-row-echelon-form-和-linear-combination的关系"><a href="#Reduced-row-echelon-form-和-linear-combination的关系" class="headerlink" title="Reduced row echelon form 和 linear combination的关系"></a>Reduced row echelon form 和 linear combination的关系</h3><p>Column correspond theorem<br>Column correspond theoremd的另外一种陈述方法<br>Ax = 0 和 Rx =0 一定会有同样的解，（为什么，因为对zero vector 做elementary operation，之后的结果仍然是zero vector）<br><a href="https://youtu.be/ObibwhRY8xc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=977" target="_blank" rel="noopener">https://youtu.be/ObibwhRY8xc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=977</a><br><a href="https://youtu.be/ObibwhRY8xc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1329" target="_blank" rel="noopener">https://youtu.be/ObibwhRY8xc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1329</a><br>重要结论：<br>column</p>
<ol>
<li>对于一个matrix A，做完reduced row echelon form之后的矩阵R，column之间的关系是不变的，column之间的承诺，不会因为elementary row operation而改变。</li>
<li>但是columns 的span会发生改变<br>row</li>
<li>the relations between the rows are changed,</li>
<li>the span of rows are the same.</li>
</ol>
<h3 id="reduced-row-echelon-form-和-Independent的关系"><a href="#reduced-row-echelon-form-和-Independent的关系" class="headerlink" title="reduced row echelon form 和 Independent的关系"></a>reduced row echelon form 和 Independent的关系</h3><p>所有不是pivot columns，它们都是出现在它们左侧的pivot column的线性组合<br>pivot column一定都是independent<br><a href="https://youtu.be/G-afSDZgEVI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=265" target="_blank" rel="noopener">https://youtu.be/G-afSDZgEVI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=265</a><br>pivot column一定是standard vector<br>为什么对于一个3<em>3的 linear indenpend的矩阵来说，一定能够化简成indentity matrix<br>因为column的correspondence不会因为做row operation发生改变，变换之前是linear independence，变换后仍然是linear independence。又因为，RREF的结果是standard vector，而且要符合RREF的定义，leading entry 要在右下边，所以结果只能是identity matrix<br><a href="https://youtu.be/G-afSDZgEVI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=316" target="_blank" rel="noopener">https://youtu.be/G-afSDZgEVI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=316</a><br>3</em>4的矩阵不可能是linear independent<br><a href="https://youtu.be/G-afSDZgEVI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=682" target="_blank" rel="noopener">https://youtu.be/G-afSDZgEVI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=682</a><br>矮胖型的矩阵一定是linear dependent（我的理解，一定有冗余信息）<br><a href="https://youtu.be/G-afSDZgEVI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=895" target="_blank" rel="noopener">https://youtu.be/G-afSDZgEVI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=895</a><br>more than m vectors in R^m must be dependent<br>这里有一个非常直观的理解——矮胖型的矩阵，就是降维打击（hhh<br><a href="https://youtu.be/G-afSDZgEVI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=959" target="_blank" rel="noopener">https://youtu.be/G-afSDZgEVI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=959</a></p>
<h3 id="reduced-row-echelon-form-和-Rank"><a href="#reduced-row-echelon-form-和-Rank" class="headerlink" title="reduced row echelon form 和 Rank"></a>reduced row echelon form 和 Rank</h3><p>什么是rank<br>就是有多少个independent的column？<br>因为Reduced row echelon form的column relationship和A的column relationship是一样的，所以RREF的rank和A的rank是一样的<br>结论：rank的数据= pivot column的数目 = Number of Non-zero rows(leading entries 的数目)<br><a href="https://youtu.be/UaBRpTMX98c?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=152" target="_blank" rel="noopener">https://youtu.be/UaBRpTMX98c?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=152</a><br>第二个结论：Rank（A） &lt;= min(number of columns , number of rows)<br><a href="https://youtu.be/UaBRpTMX98c?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=266" target="_blank" rel="noopener">https://youtu.be/UaBRpTMX98c?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=266</a><br>Basic, Free variables v.s. Rank(三者之间的关系！！！)<br>3个useful equations = 3 个basic variables  = 3个Non-zero rows = rank<br>    No. column - Non-zeor row= nullity<br><a href="https://youtu.be/UaBRpTMX98c?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=650" target="_blank" rel="noopener">https://youtu.be/UaBRpTMX98c?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=650</a><br>nullity和zero rows的数目是没有半毛钱关系的<br><a href="https://youtu.be/UaBRpTMX98c?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=810" target="_blank" rel="noopener">https://youtu.be/UaBRpTMX98c?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=810</a><br>什么是rank？</p>
<ol>
<li>rank是最大的independent column的数目</li>
<li>rank是pivot column的个数</li>
<li>rank是reduced row echelon form的non-zero rows的个数</li>
<li>rank是number of basic variables</li>
</ol>
<h3 id="reduced-row-echelon-form-和-Span"><a href="#reduced-row-echelon-form-和-Span" class="headerlink" title="reduced row echelon form 和 Span"></a>reduced row echelon form 和 Span</h3><p>有没有解（inconsistent），就看rank(A)和Rank(A,b)是不是一样的，不一样，就以为着没有解<br><a href="https://youtu.be/mSMh27SxKbM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=151" target="_blank" rel="noopener">https://youtu.be/mSMh27SxKbM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=151</a><br>为了让Ax=b一定有解，那么必须保证Rank(A) = No. of rows，为什么，请看：<br><a href="https://youtu.be/mSMh27SxKbM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=661" target="_blank" rel="noopener">https://youtu.be/mSMh27SxKbM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=661</a><br>这里有个结论，如果有m个independent的vector，你就可以span一个R^m的空间<br>接下来讨论的是full rank的矩阵有什么特性<br><a href="https://youtu.be/mSMh27SxKbM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1125" target="_blank" rel="noopener">https://youtu.be/mSMh27SxKbM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1125</a><br>对于一个瘦高的矩阵来说，如果rank = n，那么它的reduced row echelon form一定是[I ;0]<br>如果一个matrix的rank = m，那么这个矩阵一定的矮胖的矩阵;而且这个矩阵一定有解，到底有一个解，还是有无穷多个解？<a href="https://youtu.be/mSMh27SxKbM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1252" target="_blank" rel="noopener">https://youtu.be/mSMh27SxKbM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1252</a></p>
<p>第二章，讲的是，矩阵的相乘<br>我的期望得到回答的问题，为什么inner product，是向量的投影</p>
<p>矩阵相乘，其实是线性系统的组合。可以这样理解吗？</p>
<h3 id="如何理解矩阵相乘，（这样记，不会忘记）"><a href="#如何理解矩阵相乘，（这样记，不会忘记）" class="headerlink" title="如何理解矩阵相乘，（这样记，不会忘记）"></a>如何理解矩阵相乘，（这样记，不会忘记）</h3><p><a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=291" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=291</a></p>
<ol>
<li><p>角度0：高中生的观点，<br>A的row，B的column，做inner product</p>
</li>
<li><p>角度1： 从linear combination的角度看矩阵相乘<br><a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=424" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=424</a></p>
</li>
</ol>
<p>C= AB ,后边的这个B，可以看成很多个输入，<br><a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=726" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=726</a></p>
<p>相乘这件事，可以看成两个线性linear system的composition<br>那么什么是composition？<br><a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=753" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=753</a></p>
<p>【g。f】的（圈圈是中间的位置）含义是，输入x，先经过f，然后经过g<a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=844" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=844</a></p>
<ol start="3">
<li>角度2：从linear combination of rows<br>从行的角度考虑，两个矩阵相乘。<br><a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1306" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1306</a></li>
</ol>
<ol start="4">
<li>角度3，矩阵相乘，可以看做是summation of matrices<br><a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1560" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1560</a><br>矩阵相乘，可以看做是一堆rank是1的矩阵，相加</li>
</ol>
<p>Block Multiplication,用这种方法可以简化矩阵的计算过程<a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2492" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2492</a></p>
<p>矩阵相乘的性质，<br>AB 不等于 BA<br><a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2893" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2893</a></p>
<h3 id="AC-T-？-A乘上C的转置是什么"><a href="#AC-T-？-A乘上C的转置是什么" class="headerlink" title="(AC)^T = ？ (A乘上C的转置是什么)"></a>(AC)^T = ？ (A乘上C的转置是什么)</h3><p><a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3342" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3342</a></p>
<p>(AC)^T = C^T A^T</p>
<h3 id="一些特殊的matrix"><a href="#一些特殊的matrix" class="headerlink" title="一些特殊的matrix"></a>一些特殊的matrix</h3><p>Diagonal Matrix<br>    对角矩阵<br>Symmetric Matrix<br>    A^T = A<br><a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3486" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3486</a></p>
<p>矩阵相乘，不同的顺序，可能导致不同的运算量<br><a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW</a> &amp;t=3647</p>
<h3 id="什么是矩阵的逆-what’s-the-inverse-of-matrix"><a href="#什么是矩阵的逆-what’s-the-inverse-of-matrix" class="headerlink" title="什么是矩阵的逆(what’s the inverse of matrix)"></a>什么是矩阵的逆(what’s the inverse of matrix)</h3><p>matrix inverse<br>类比，什么是一个function的 inverse。<br>把什么东西丢到f里边，都能够通过g还原回来，<br>同样的，把什么东西丢到g里边，都能够通过f还原回来。<a href="https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=171" target="_blank" rel="noopener">https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=171</a></p>
<p>什么叫做A和B互为inverse，什么叫做B和A互为inverse<br>为什么，<br>说法1：对于任何一个Vector v， 先丢到A里边，再丢到B里边，得到的输出和输入一样。同理，对于任何一个Vector v，先丢到B里边，再丢到A里边，得到的输出和输入一样，那么A和B，这两个矩阵就是互为inverse。<br>说法2<br>AB = I;<br>BA = I;<br>那么A跟B就互为inverse；</p>
<p>这两个说法是一致的，如何理解？<br>因为矩阵相乘说的就是，把两个系统compose起来，A和B乘一起，就等同于得到一个新的function，这个新的function，输入什么就输入什么，那这样的系统是什么呢？就是Identity matrix<br><a href="https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=317" target="_blank" rel="noopener">https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=317</a></p>
<p>以下是课本上的定义：<br><a href="https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=406" target="_blank" rel="noopener">https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=406</a></p>
<h3 id="这一引入新的概念，singular，和non-singular。"><a href="#这一引入新的概念，singular，和non-singular。" class="headerlink" title="这一引入新的概念，singular，和non-singular。"></a>这一引入新的概念，singular，和non-singular。</h3><p>如果一个矩阵是invertible的，那么就说这个矩阵是Non-Singular<br>为什么叫Non-Singular呢？<br>因为Singular是单一的，不是成双成对的，如果A有一个inverse，有一个伴，就说明它是Non-Singular。<br><a href="https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=493" target="_blank" rel="noopener">https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=493</a></p>
<p>有这样一个有趣的事实，在讨论一个matrix是不是invertible，都讨论的是Square Matrix。<br>为什么？<br>直观的理解，你把高纬度的东西降低到低纬度上后，你无法再从第维度上，把高纬度的信息还原回来的<br><a href="https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=782" target="_blank" rel="noopener">https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=782</a></p>
<p>并不是所有square的matrix都是可逆的<br><a href="https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1135" target="_blank" rel="noopener">https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1135</a></p>
<p>可逆这件事是唯一的。<br>这样理解，这个世界上有很多矩阵，有一些是有伴侣（Non-Singular），剩下的就是没有伴侣的(Singular)，有伴侣的矩阵，都是有唯一的伴侣。<br><a href="https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1297" target="_blank" rel="noopener">https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1297</a></p>
<p>如果A和B都是invertible，那么AB也是invertible的，这里是证明<br><a href="https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1429" target="_blank" rel="noopener">https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1429</a></p>
<p>(A^T)^-1 = (A^-1)^T，很神奇的一个步骤，为什么？<br>利用定义证<br><a href="https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1501" target="_blank" rel="noopener">https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1501</a></p>
<p>这里引申一个问题，<br>为什么，(AB)^T = B^TA^T<br>从矩阵的维度上回答这个问题<br><a href="https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3382" target="_blank" rel="noopener">https://youtu.be/yO8lDzf4jMs?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3382</a></p>
<p>利用inverse来解，线性方程组，请看<br>其实这种方法有点舍本逐末。因为计算inverse of A的时候需要计算 reduced row echelon form<br><a href="https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1708" target="_blank" rel="noopener">https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1708</a></p>
<p>观察inverse of A能够学到一些东西，什么东西，很好奇！？<br><a href="https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1861" target="_blank" rel="noopener">https://youtu.be/fOK-bLERPUM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1861</a></p>
<p>接来下要问的问题是，什么样的matrix有inverse？（什么样的矩阵是Non-Singular）<br><a href="https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=30" target="_blank" rel="noopener">https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=30</a> </p>
<p>一个square matrix 是invertible的，有十种等价的描述！！！【头大】<br><a href="https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=221" target="_blank" rel="noopener">https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=221</a></p>
<p>复习，One-to-one<br>如果一个矩阵是矮胖型的矩阵，那么它的input domain就比较大， output domain就比较小。那么它对应的function就不可能是one-to-one的<br><a href="https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=508" target="_blank" rel="noopener">https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=508</a></p>
<p>如果co-domain比较大，也不可能是one-to-one的。</p>
<p>总结：一个矩阵是one-to-one的，那么它的column是independent；</p>
<p>复习，Onto<br><a href="https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=804" target="_blank" rel="noopener">https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=804</a><br>总结，如果一个matrix是onto的，那么满足什么样的性质呢？<br>rank A = no. of rows</p>
<p>如果一个function，同时one-to-one，又是onto的，意味着什么呢？<br>意味着，input domain和co-domain是一样大的。<br>如何证明？<br><a href="https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1087" target="_blank" rel="noopener">https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1087</a></p>
<p>one-to-one就是一个萝卜一个坑的意思（太形象了）<br>onto的意思，是所有的坑里都有放萝卜。</p>
<p><a href="https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1177" target="_blank" rel="noopener">https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1177</a></p>
<p>one-to-one和Onto是一个意思<br><a href="https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1254" target="_blank" rel="noopener">https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1254</a></p>
<h3 id="一个矩阵是不是可逆的，10种等价说法，"><a href="#一个矩阵是不是可逆的，10种等价说法，" class="headerlink" title="一个矩阵是不是可逆的，10种等价说法，"></a>一个矩阵是不是可逆的，10种等价说法，</h3><p>分类来记忆<br><a href="https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1420" target="_blank" rel="noopener">https://youtu.be/d43mGvCnuBU?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1420</a><br>一类是检查是不是onto，一类是检查是不是ont-to-one</p>
<p>最简单的方法是什么？直接检查reduced row echelon form是不是Indentity matrix</p>
<p>今天的主题是，How to find the Inverse of a Matrix<br><a href="https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=8" target="_blank" rel="noopener">https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=8</a></p>
<p>一种特殊的matrix（elementary matrix）<br>实际上Elementary row operation都可以表示成一个matrix。<br><a href="https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=172" target="_blank" rel="noopener">https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=172</a></p>
<p>所以，我们学到什么事情呢？<br>elementary matrix其实是干一件事情（互换、scaling、相加adding k times row i to row j），把这个作用作用到Indentity matrix上就能得到这个elementary matrix<br><a href="https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=293" target="_blank" rel="noopener">https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=293</a></p>
<p>接下来要告诉你的事情是，这些elementary matrix，你可以轻易地找到它们的reverse；<br><a href="https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=712" target="_blank" rel="noopener">https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=712</a></p>
<p>reduced row echelon form 这个操作可以换成一连串的matrix 操作<br><a href="https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=769" target="_blank" rel="noopener">https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=769</a></p>
<p>之前说过，把一堆invertible的matrix相乘，它们的结果仍然是invertible的。（说过吗？我已经没有印象了，如果证明，P-1 * P  = I ）<br>PA = R<br>P = Ek···E2 E1</p>
<p>如果一个矩阵A是invertible的，那么它的RREF是In，等价于说，A是一连串的elementary matrix的乘机，为什么？<br><a href="https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=940" target="_blank" rel="noopener">https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=940</a></p>
<p>如何求解一个矩阵A的inverse？</p>
<p>构造一个[A I_n]<br><a href="https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1514" target="_blank" rel="noopener">https://youtu.be/vV2ff0xFPbw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1514</a></p>
<p>从今天起，开始学习第四章</p>
<h3 id="Subspace"><a href="#Subspace" class="headerlink" title="Subspace"></a>Subspace</h3><p><a href="https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=6" target="_blank" rel="noopener">https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=6</a></p>
<p>有一些vector set可以叫做subspace</p>
<p>这一章讲的什么事情呢？<br>讲的是，同一个function、operator，matrix，从不同的观点看就是不同的事，观点就是coordinate system.</p>
<p>有一些Vector可以被叫做subspace<br><a href="https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=277" target="_blank" rel="noopener">https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=277</a></p>
<p>要满足三个条件（首先它是一个vector set），</p>
<ol>
<li>第一条，zero vector 一定要在里边。</li>
<li>u+v一定也在V里边</li>
<li>u，乘上c，得到cu，也一定在V里边</li>
</ol>
<p>subspace 和 span有什么关系呢？<br>span所产生的vector set一定是subspace。<br>每一个subspace都可以看成是由一组vector 做span所产生的。<br><a href="https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1417" target="_blank" rel="noopener">https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1417</a></p>
<h3 id="Null-Space：所有Ax-0-的solution所构成的集合就是Null-A"><a href="#Null-Space：所有Ax-0-的solution所构成的集合就是Null-A" class="headerlink" title="Null Space：所有Ax = 0 的solution所构成的集合就是Null A"></a>Null Space：所有Ax = 0 的solution所构成的集合就是Null A</h3><p>Null A = {v ∈ R^n:Av = 0}</p>
<h3 id="Column-Space"><a href="#Column-Space" class="headerlink" title="Column Space"></a>Column Space</h3><p>A ∈ R^{m*n} ==&gt;  Col A = {Av:v∈R^n}</p>
<p>Column space = range，如何理解？把A当成一个function，任意的输入对应的输出，构成的集合</p>
<p><a href="https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1722" target="_blank" rel="noopener">https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1722</a></p>
<p>这里有一个需要注意的结论：<br>根据column combination theory，column的关系不会发生改变，但是Col A ≠ Col R<br>为什么？<br>但是 Row A = Row R<br><a href="https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1880" target="_blank" rel="noopener">https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1880</a></p>
<p>Consistent就是有解得意思，<br>有解，现在有4种描述方式了<br><a href="https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1967" target="_blank" rel="noopener">https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1967</a></p>
<p>如果A的augmented matrix 的RREF中，有某一行，只有最后一个维度有值，那么它就是没有解得。<br><a href="https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2059" target="_blank" rel="noopener">https://youtu.be/pXtXnY2b2-E?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2059</a></p>
<p>这节课的结论：<br>Conclusion ： Subspace is closed under addition and multiplication</p>
<h3 id="Basis（基础、支撑）"><a href="#Basis（基础、支撑）" class="headerlink" title="Basis（基础、支撑）"></a>Basis（基础、支撑）</h3><p><a href="https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=17" target="_blank" rel="noopener">https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=17</a></p>
<ol>
<li>什么是subspace的basis<br><a href="https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=122" target="_blank" rel="noopener">https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=122</a><br>一个vector set满足以下两个条件<br>(1) linearly independent<br>(2) genenration set of V</li>
</ol>
<p>请复习，什么是subspace；（Subspace is closed under addition and multiplication，subspace是一个vector set，在里边的vector做线linear combination后得到的向量，仍然在这里边。同时这个vector set 包含zero vector，这两的 是vector set就是subspace）</p>
<p>A的pivot columns是Col A space的basis<br>为什么？<br><a href="https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=466" target="_blank" rel="noopener">https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=466</a></p>
<p>问题:每一个pivot column都要包含在 basis里边吗？（<a href="https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=912）" target="_blank" rel="noopener">https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=912）</a><br>回答：pivot column是linear independent的，所以，你少掉其中一个，那今天，你构造column space的时候是有包含你少掉的那个pivot column，你用其它的pivot column做linear combination是没办法得到你少掉的那个pivot column，所以pivot column是缺一不可的。</p>
<p>basis is always in its subspace。</p>
<h3 id="下来讲basis的重要的三个定理"><a href="#下来讲basis的重要的三个定理" class="headerlink" title="下来讲basis的重要的三个定理"></a>下来讲basis的重要的三个定理</h3><p>theorem<br><a href="https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1068" target="_blank" rel="noopener">https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1068</a></p>
<ol>
<li>basis是最小的generation set。</li>
<li>basis又是最大的independent vector set </li>
<li>一个subspace，可以有无穷多个basis。但是这些basis里边的vector的数量是一样多的。</li>
</ol>
<p>接下里是3的证明<br><a href="https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1110" target="_blank" rel="noopener">https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1110</a></p>
<p>reduction theorem<br>每一个generation set 里边都有一个basis。</p>
<p>extension theorem</p>
<p>一个Independent set：我不是一个basis，就是一个正在成为一个basis<br>所以，所有的independent vector 一定包含在basis里边<br><a href="https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2778" target="_blank" rel="noopener">https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2778</a></p>
<p>麻将是4维空间中的游戏（hhhh）：<a href="https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3019" target="_blank" rel="noopener">https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3019</a></p>
<p>线性代数告诉我们的，如果你组一个队伍，如果有四个人，其中有一个人是多余的：<a href="https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3074" target="_blank" rel="noopener">https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3074</a></p>
<p>summary ：<br>把basis，想成一个雕像：<br><a href="https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3135" target="_blank" rel="noopener">https://youtu.be/GB48DyvC14o?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3135</a></p>
<ol start="2">
<li>怎么检查一个东西是不是这个subspace的basis<br>接下来的内容是，如果给你一个vector set，怎么confirm它是不是一个basis</li>
</ol>
<p>检查一个vector set 是不是generation set<br>检查两件事（1）坚持subspace的dim，（2）检查independent</p>
<p>接下来的内容是：讲一些耳熟能详、有名有姓的subspace 的特性。（讲3个）<br><a href="https://youtu.be/aW0JVmpIxas?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=9" target="_blank" rel="noopener">https://youtu.be/aW0JVmpIxas?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=9</a></p>
<p>matrix的 col A(col space )<br>    1. columns space的dim = A 的rank</p>
<p>matrix的 null A(null space )</p>
<p>matrix的 row A(row space )</p>
<p>怎么找它们的basis<br>怎么找它们的dim<br><a href="https://youtu.be/aW0JVmpIxas?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=247" target="_blank" rel="noopener">https://youtu.be/aW0JVmpIxas?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=247</a></p>
<p>Rank(A)意味着什么？<br><a href="https://youtu.be/aW0JVmpIxas?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=410" target="_blank" rel="noopener">https://youtu.be/aW0JVmpIxas?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=410</a></p>
<p>range的dim就是column space的dim，两者是相等的</p>
<p>这节课的summary<br><a href="https://youtu.be/aW0JVmpIxas?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1255" target="_blank" rel="noopener">https://youtu.be/aW0JVmpIxas?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1255</a></p>
<p>接下来讲，coordinate system</p>
<p>coordinate systems是什么呢？是拿来描述vector的观点<br><a href="https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=99" target="_blank" rel="noopener">https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=99</a></p>
<p>如何从一个coordinate system换到另外一个coordinate system</p>
<p>由standard vector组成的coordinate system，叫做直角坐标系<br><a href="https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=238" target="_blank" rel="noopener">https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=238</a></p>
<p>什么样的vector set 能够拿来当coordinate system？<br><a href="https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=570" target="_blank" rel="noopener">https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=570</a><br>basis—-<br>为什么是basis？<br><a href="https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=813" target="_blank" rel="noopener">https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=813</a></p>
<p>v代表什么？代表的是在直角坐标系中看到的vector<br><a href="https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1194" target="_blank" rel="noopener">https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1194</a><br>直角坐标系：<br>cartesian coordinate system(直角坐标系)</p>
<p>其它的坐标系转换到直角坐标系<br>other system  ——&gt; Cartesian<br>v = B[v]_B<br><a href="https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1435" target="_blank" rel="noopener">https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1435</a></p>
<p>Cartesian ——&gt; other system</p>
<p>[v]_B = B^{-1}v</p>
<h3 id="怎么在两个坐标系中，做任意的转换？"><a href="#怎么在两个坐标系中，做任意的转换？" class="headerlink" title="怎么在两个坐标系中，做任意的转换？"></a>怎么在两个坐标系中，做任意的转换？</h3><p><a href="https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1688" target="_blank" rel="noopener">https://youtu.be/im3kTm9jGEM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1688</a></p>
<p>接下来讨论的问题是：<br>Linear Function in Coordinate System<br><a href="https://youtu.be/IrAdVhE6VqI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=27" target="_blank" rel="noopener">https://youtu.be/IrAdVhE6VqI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=27</a><br>同一个function，在不同的coordinate system里边看起来都是不一样的</p>
<p>为什么，我们要在不同的coordinate system里边描述同一个function呢？</p>
<p>在直角坐标系统，有一个复杂的function。在新的坐标系中，这个function就会变得简单。<br><a href="https://youtu.be/IrAdVhE6VqI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=210" target="_blank" rel="noopener">https://youtu.be/IrAdVhE6VqI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=210</a></p>
<p>Descrbing the function in another coordinate system</p>
<p>[T]_B在B的coordinate system中观察function T<br><a href="https://youtu.be/IrAdVhE6VqI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=652" target="_blank" rel="noopener">https://youtu.be/IrAdVhE6VqI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=652</a> </p>
<p>这部分讲的内容是，从新的coordinate system的角度去观察function。比如在现在的坐标系中，你的函数很复杂，那么你能不能换个角度去观察这个问题呢？显然是可以的——用的coordinate system。<br><a href="https://youtu.be/IrAdVhE6VqI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1287" target="_blank" rel="noopener">https://youtu.be/IrAdVhE6VqI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1287</a></p>
<h3 id="这里引入一个新的概念similar；"><a href="#这里引入一个新的概念similar；" class="headerlink" title="这里引入一个新的概念similar；"></a>这里引入一个新的概念similar；</h3><p><a href="https://youtu.be/IrAdVhE6VqI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1283" target="_blank" rel="noopener">https://youtu.be/IrAdVhE6VqI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1283</a><br>如何看这两个matrix呢？<br>A是一个linear transform，A’是另外一个坐标系下看待的linear transform。</p>
<p>关系是，是同一个linear transform，但是是在不同的世界（不同的coordinate system ）下看。</p>
<p>是同一个linear system，但是是在不同的坐标系看待，什么样的联系？</p>
<p>这节课的结论：<br><a href="https://youtu.be/IrAdVhE6VqI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW" target="_blank" rel="noopener">https://youtu.be/IrAdVhE6VqI?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW</a></p>
<p>接下来讲的是行列式determinant<br><a href="https://youtu.be/7fXtSUrKND0?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2" target="_blank" rel="noopener">https://youtu.be/7fXtSUrKND0?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2</a></p>
<p>给你一个matrix，都会对应到一个数值。根据这个值，你就能判断出关于这个matrix的一些性质</p>
<p>先学习一些cofactor expansion<br><a href="https://youtu.be/7fXtSUrKND0?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=429" target="_blank" rel="noopener">https://youtu.be/7fXtSUrKND0?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=429</a></p>
<p>接下来要分析的内容是，properties of Determinats<br>可以视为，高维空间的体积<br><a href="https://youtu.be/7fXtSUrKND0?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1531" target="_blank" rel="noopener">https://youtu.be/7fXtSUrKND0?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1531</a></p>
<p>determinant 有这三个性质<br><a href="https://youtu.be/005nG8ZZVDE?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW" target="_blank" rel="noopener">https://youtu.be/005nG8ZZVDE?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW</a></p>
<p>这里有一个逻辑上的推导，<br>cofactor这么复杂，是怎么来的呢？<br>是由上边的三个性质推导出来的。</p>
<p><a href="https://youtu.be/005nG8ZZVDE?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=242" target="_blank" rel="noopener">https://youtu.be/005nG8ZZVDE?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=242</a></p>
<p>det对row是线性的，是一种很特殊的“线性”关系</p>
<p>有一个新的矩阵叫upper triangular matrix</p>
<p>这里有一个非常重要的结论<br>如果一个矩阵是invertible的，若且为若，det(A)≠0<br><a href="https://youtu.be/005nG8ZZVDE?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1689" target="_blank" rel="noopener">https://youtu.be/005nG8ZZVDE?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1689</a> </p>
<p>这里复习一下，invertible，意味着什么，这里一共有11中描述方法，也就是说至少有11个视角<br><a href="https://youtu.be/005nG8ZZVDE?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1898" target="_blank" rel="noopener">https://youtu.be/005nG8ZZVDE?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1898</a></p>
<p>讲cramer’s rule,克莱姆法则<br><a href="https://youtu.be/005nG8ZZVDE?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2283" target="_blank" rel="noopener">https://youtu.be/005nG8ZZVDE?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2283</a> </p>
<p>了解一下adjugate of A</p>
<p>第五章</p>
<h3 id="从现在起，就进入eigenXXX的世界"><a href="#从现在起，就进入eigenXXX的世界" class="headerlink" title="从现在起，就进入eigenXXX的世界"></a>从现在起，就进入eigenXXX的世界</h3><p><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=7" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=7</a></p>
<p>第四章，我们提到，我们可以换一个coordinate system，看一个linear system，让这个系统看起来很简单。但是我们没有讲，如何找一个这样的coordinate system</p>
<p><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=49" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=49</a></p>
<p>什么是eigenvalue、eigenvector<br>什么是eigen（German word，代表unique to， belong to）<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=167" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=167</a></p>
<p>这里有一个前提，</p>
<ol>
<li>我们讨论eigen的时候，我们的矩阵必须是suqare</li>
<li>我们是不讨论Zero vector</li>
</ol>
<p>每一个matrix都对应一个linear function（或者叫linear operator）</p>
<p>这个例子告诉我们，一个matrix有两个eigenvector<br>这个例子告诉我们，任何一个vector都是eigenVector<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=760" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=760</a></p>
<h3 id="并不是所有的matrix都有eigenvector"><a href="#并不是所有的matrix都有eigenvector" class="headerlink" title="并不是所有的matrix都有eigenvector"></a>并不是所有的matrix都有eigenvector</h3><p>每一个eigenvector都对应到一个unique的eigenvalue<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1120" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1120</a></p>
<h3 id="同一个eigenvalue会对应到很多的eigenvector"><a href="#同一个eigenvalue会对应到很多的eigenvector" class="headerlink" title="同一个eigenvalue会对应到很多的eigenvector"></a>同一个eigenvalue会对应到很多的eigenvector</h3><p>问题：同一个eigenvalue，它对应的eigenvector，所形成的vector set是不是一个subspace？</p>
<p>复习，subspace的定义<br>（我的理解：一个结界，跑不出去）<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1323" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1323</a></p>
<p>它不是一个subspace，因为，它没有zero-vector</p>
<p>eigenspace的定义<br>eigenvectors corresponding to lamda + {0}<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1537" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1537</a></p>
<p>确定一个scalar是不是eigenvalue?<br>很简单，你看看它的eigenspace，如果只有zero vector，那么就不是，否则就是<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1761" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1761</a> </p>
<p>这里需要我们注意，nullity space也是一个很重要的space<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1813" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1813</a></p>
<p>How to find eigenvectors(given eigenvalues)<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1097" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1097</a></p>
<ol>
<li>每一个eigenvector对应到（corresponds） 一个unique的eigenvalue</li>
<li>一个eigenvalues有无穷多个eigenvectors</li>
</ol>
<p>Eigenvectors corresponding to lamda are nonzero sulution of (A-lamdaI_n)v = 0</p>
<h3 id="eigenspace-就是-A-lamdaI-n-的null-space"><a href="#eigenspace-就是-A-lamdaI-n-的null-space" class="headerlink" title="eigenspace 就是(A-lamdaI_n)的null space"></a>eigenspace 就是(A-lamdaI_n)的null space</h3><p>lamda的eigenspace = 和lamda相关的eigenvectors + {0}<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1725" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1725</a></p>
<p>如何确定一个scalar是不是eigenvalue?<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1733" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1733</a></p>
<p>就看它的eigenspace长什么样子，看看它的eigenspace的dim</p>
<p>如何看一个null space的dim???<br>（回忆之前检查subspace的大小的方法，看matrix的independent）<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1860" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1860</a></p>
<p>如何找eigenspace的basis?<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2112" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2112</a></p>
<p>如何从一个matrix到parametric representation<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2248" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2248</a></p>
<p>如何寻找eigenvalue?(Looking for eigenvalues)<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1818" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1818</a></p>
<p>什么是trace（把对角线相加）<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2620" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2620</a></p>
<p>什么是polynomial?<br>characteristic polynomial<br>characteristic equation</p>
<p><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3123" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3123</a></p>
<h3 id="characteristic的特性"><a href="#characteristic的特性" class="headerlink" title="characteristic的特性"></a>characteristic的特性</h3><p>讲到eigenvalue之后呢reduced row echelon  form作用就不大了，因为，一个matrix的characteristic polynomial和它的RREF的characteristic polynomial是不一样的<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3237" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3237</a></p>
<p>如何两个matrix是similar的，RREF和它本身有相同的character polynomial<br>意味着有用同样的eigenvalue.</p>
<h3 id="什么是similar"><a href="#什么是similar" class="headerlink" title="什么是similar?"></a>什么是similar?</h3><p>是同一个东西，在不同的坐标系(coordinate system)下看<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3731" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3731</a></p>
<p>cha-ra-te-ris-tic po-ly-no-mial的degree（order）是n<br>cha-ra-te-ris-tic<br>charateristic charateristic charateristic charateristic charateristic charateristic charateristic charateristic charateristic charateristic charateristic charateristic<br>接下来的问题是，这个N*n 的matrix会有多少个eigenvalue呢？<br>最多有n个根，</p>
<p>一些神奇的内容：<br><a href="https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=4282" target="_blank" rel="noopener">https://youtu.be/1RyHRIP8QGg?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=4282</a></p>
<p>接下来要讲的内容是Diagonalization<br>Di-a-go-na-li-za-tion<br>Di-a-go-na-li-z-able（可对角化的）<br><a href="https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=24" target="_blank" rel="noopener">https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=24</a></p>
<ol>
<li>有一些矩阵可以做diagonalization<br>A = PDP^{-1}   </li>
<li>如何找到D和P</li>
</ol>
<p>diagonazation有什么重要性呢？<br><a href="https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=81" target="_blank" rel="noopener">https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=81</a><br>如果一个矩阵是可对角化的，那么意味着它和某一个矩阵是相似的。相似意味着是同一个linear operation在不同的坐标系下观察。</p>
<p>因为对角化的矩阵很容易分析，所以这是简化问题的一个关键技能。</p>
<p><a href="https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=147" target="_blank" rel="noopener">https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=147</a></p>
<h3 id="Diagonalizable和eigenvalue、eigenvector有关系"><a href="#Diagonalizable和eigenvalue、eigenvector有关系" class="headerlink" title="Diagonalizable和eigenvalue、eigenvector有关系"></a>Diagonalizable和eigenvalue、eigenvector有关系</h3><p>pi就是eigenvector，di就是eigenvalue<br><a href="https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=470" target="_blank" rel="noopener">https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=470</a></p>
<h3 id="如何找到一个matrix的eigenvector和eigenvalue"><a href="#如何找到一个matrix的eigenvector和eigenvalue" class="headerlink" title="如何找到一个matrix的eigenvector和eigenvalue?"></a>如何找到一个matrix的eigenvector和eigenvalue?</h3><p><a href="https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=807" target="_blank" rel="noopener">https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=807</a></p>
<h3 id="对于一个matrix先找到它的eigenvalue-然后就能找到这个eigenvalue对应eigenspace。eigenspace除了-0-之外，都是eigenvector"><a href="#对于一个matrix先找到它的eigenvalue-然后就能找到这个eigenvalue对应eigenspace。eigenspace除了-0-之外，都是eigenvector" class="headerlink" title="对于一个matrix先找到它的eigenvalue,然后就能找到这个eigenvalue对应eigenspace。eigenspace除了{0}之外，都是eigenvector"></a>对于一个matrix先找到它的eigenvalue,然后就能找到这个eigenvalue对应eigenspace。eigenspace除了{0}之外，都是eigenvector</h3><p>对角化，必须找到n个independent的eigenvector</p>
<p>如何对角化一个，matrix?<br><a href="https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1350" target="_blank" rel="noopener">https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1350</a></p>
<p>factorization（因式分解）<br>对应到不同的eigenvalue的eigenvector<br>都是independent的<br><a href="https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1542" target="_blank" rel="noopener">https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1542</a><br>从同一个eigenspace取出来的两个eigenvector有可能是independent<br><a href="https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1594" target="_blank" rel="noopener">https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1594</a></p>
<p>不同的eigenvalue对应的eigenvector一定是independent，的证明<br><a href="https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1938" target="_blank" rel="noopener">https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1938</a> </p>
<p>有没有办法进行对角化，这里有一个推论<br><a href="https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2084" target="_blank" rel="noopener">https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2084</a></p>
<p>这里是一个例子，求一个矩阵的对角化矩阵<br>diagonalizable<br><a href="https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2264" target="_blank" rel="noopener">https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2264</a></p>
<h3 id="对角化有什么应用？"><a href="#对角化有什么应用？" class="headerlink" title="对角化有什么应用？"></a>对角化有什么应用？</h3><p><a href="https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2603" target="_blank" rel="noopener">https://youtu.be/TsB5_BiMFoo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2603</a> </p>
<h3 id="如何知道一个矩阵有没有办法被对角化"><a href="#如何知道一个矩阵有没有办法被对角化" class="headerlink" title="如何知道一个矩阵有没有办法被对角化"></a>如何知道一个矩阵有没有办法被对角化</h3><p><a href="https://youtu.be/L7Y8wB3xzEc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=11" target="_blank" rel="noopener">https://youtu.be/L7Y8wB3xzEc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=11</a></p>
<p>对一个linear function做diagonalization</p>
<p><a href="https://youtu.be/L7Y8wB3xzEc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1028" target="_blank" rel="noopener">https://youtu.be/L7Y8wB3xzEc?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1028</a></p>
<p>pageRank很长时间会被updated一次，这节课讲的就是pagerank(看的不是网页的内容，而是有多少其它的网页link到你的网页上，整个网络的结构)<br><a href="https://youtu.be/pSg9TG_U_fY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=13" target="_blank" rel="noopener">https://youtu.be/pSg9TG_U_fY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=13</a></p>
<p>如果一个矩阵的column的和都为1，那么它一定有一个eigenvalue为1<br><a href="https://youtu.be/pSg9TG_U_fY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=852" target="_blank" rel="noopener">https://youtu.be/pSg9TG_U_fY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=852</a> </p>
<p>这个解是唯一的吗？（PageRank的分数是）<br><a href="https://youtu.be/pSg9TG_U_fY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1019" target="_blank" rel="noopener">https://youtu.be/pSg9TG_U_fY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1019</a></p>
<p>这个矩阵M = （1-m）A + mS的eigenValue是unique<br><a href="https://youtu.be/pSg9TG_U_fY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1619" target="_blank" rel="noopener">https://youtu.be/pSg9TG_U_fY?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1619</a></p>
<p>第七章—————————–&gt;<br>接下来学习的是：Orthogonality<br><a href="https://youtu.be/hxI7stenqaw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=36" target="_blank" rel="noopener">https://youtu.be/hxI7stenqaw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=36</a></p>
<p>定义的一些名词：<br>norm什么意思？——一个vector的长度||V||<br>Distance，距离||v-u||<br><a href="https://youtu.be/hxI7stenqaw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=209" target="_blank" rel="noopener">https://youtu.be/hxI7stenqaw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=209</a></p>
<p>Orthogonal</p>
<p>Dot product(点乘)</p>
<p>如果两个vector做dot product之后它们是0，那么这两个vector是orthogonal</p>
<p>dot product的性质：<br><a href="https://youtu.be/hxI7stenqaw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=688" target="_blank" rel="noopener">https://youtu.be/hxI7stenqaw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=688</a></p>
<p>一个定理：<br>pythagorean theorem（毕达哥拉斯定理）<br><a href="https://youtu.be/hxI7stenqaw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1008" target="_blank" rel="noopener">https://youtu.be/hxI7stenqaw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1008</a></p>
<p>三角不等式：<br><a href="https://youtu.be/hxI7stenqaw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1402" target="_blank" rel="noopener">https://youtu.be/hxI7stenqaw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1402</a></p>
<p>能不能证明一下柯西不等式<br><a href="https://youtu.be/hxI7stenqaw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1531" target="_blank" rel="noopener">https://youtu.be/hxI7stenqaw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1531</a></p>
<p>7.3-orthogonal projection正交投影<br><a href="https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=8" target="_blank" rel="noopener">https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=8</a></p>
<ol>
<li>什么是orthogonal complement</li>
</ol>
<p>S perb一定是一个subspace吗？<br><a href="https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=595" target="_blank" rel="noopener">https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=595</a></p>
<p>讲一个很重要的事情：<br><a href="https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=929" target="_blank" rel="noopener">https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=929</a><br>为什么要讲perb呢？<br>因为，任何一个vector，都可以拆解成u = w + z;而且，这个拆解是唯一的<br><a href="https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2459" target="_blank" rel="noopener">https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2459</a></p>
<p>Orthogonal projection<br><a href="https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2542" target="_blank" rel="noopener">https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2542</a><br>U_w(u)</p>
<p>Orthogonal projection is linear operation</p>
<p>为什么w和u的距离是最短呢？<br><a href="https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2880" target="_blank" rel="noopener">https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2880</a></p>
<p>orthogonal projection matrix<br>如何找出这个矩阵P_w<br><a href="https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3201" target="_blank" rel="noopener">https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=3201</a></p>
<p>李老师教你，如何记住Matrix C的orthogonomal projection matrix<br>（特别形象，天线宝宝中的丁丁长的就是这个矩阵）<br>matrix C代表什么啊？_代表的就是我们已知的matrix</p>
<p>matrix w代表一个subspace的W<br><a href="https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=4415" target="_blank" rel="noopener">https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=4415</a> </p>
<p>如何考虑投影这件事？<br>投影，是一个vector v投影到一个subspace里边【在subspace W中找到一个vector w，它距离 v最近】</p>
<h3 id="orthogonal-projection-到底有什么用呢？"><a href="#orthogonal-projection-到底有什么用呢？" class="headerlink" title="orthogonal projection 到底有什么用呢？"></a>orthogonal projection 到底有什么用呢？</h3><p>假设现在没有解，那你能不能给我找一个最相似的解呢？<br><a href="https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=4740" target="_blank" rel="noopener">https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=4740</a></p>
<h3 id="最小而成近似-least-square-approximation"><a href="#最小而成近似-least-square-approximation" class="headerlink" title="最小而成近似 least square approximation"></a>最小而成近似 least square approximation</h3><p><a href="https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=4912" target="_blank" rel="noopener">https://youtu.be/6WJikUaKKNo?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=4912</a><br>希望误差越小越好，这句话到底有什么的意思呢？<br>等同于vector e的norm尽可能的小、<br>e = y - a_0 v-1 - a_1 v_2 = y - C a</p>
<h3 id="Orthogonal-basis（今天讲的是）"><a href="#Orthogonal-basis（今天讲的是）" class="headerlink" title="Orthogonal basis（今天讲的是）"></a>Orthogonal basis（今天讲的是）</h3><p><a href="https://youtu.be/98-0Q1ed3sM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=4" target="_blank" rel="noopener">https://youtu.be/98-0Q1ed3sM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=4</a></p>
<ol>
<li>定义什么是orthogonal basis</li>
</ol>
<p>定义什么是orthogonal set<br><a href="https://youtu.be/98-0Q1ed3sM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=64" target="_blank" rel="noopener">https://youtu.be/98-0Q1ed3sM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=64</a></p>
<p>orthogonal set一定是independent的吗？<br>答案是：不是的</p>
<p>orthonormal set()<br>里边的长度都是1；</p>
<p>orthogonal decomposition theory()<br><a href="https://youtu.be/98-0Q1ed3sM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=672" target="_blank" rel="noopener">https://youtu.be/98-0Q1ed3sM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=672</a></p>
<p>假设今天你的basis是orthogonal的，那么今天你就有一个比较简洁的式子，可以算出一个vector转到新的空间上边，它长什么样子<br><a href="https://youtu.be/98-0Q1ed3sM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=782" target="_blank" rel="noopener">https://youtu.be/98-0Q1ed3sM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=782</a></p>
<p>如何为一个subspace找一个orthogonal的basis<br>我觉得非常直观<br>Gram-Schmidt process<br><a href="https://youtu.be/98-0Q1ed3sM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2161" target="_blank" rel="noopener">https://youtu.be/98-0Q1ed3sM?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2161</a></p>
<h3 id="如何把一组independent-vector-变成orthogonal-basis呢？"><a href="#如何把一组independent-vector-变成orthogonal-basis呢？" class="headerlink" title="如何把一组independent vector 变成orthogonal basis呢？"></a>如何把一组independent vector 变成orthogonal basis呢？</h3><p><a href="https://youtu.be/PzqVLldlHTE?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=87" target="_blank" rel="noopener">https://youtu.be/PzqVLldlHTE?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=87</a><br>span{v1,v2,···,} = span{u1,u2,···}</p>
<p>归纳法能够来证明这件事儿</p>
<p>接下来讨论的是<br>orthogonal matrix 和 symmetric matrix<br><a href="https://youtu.be/TmDYxL7HV68?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=28" target="_blank" rel="noopener">https://youtu.be/TmDYxL7HV68?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=28</a></p>
<p>Norm-preserving<br>长度不变<br><a href="https://youtu.be/TmDYxL7HV68?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=295" target="_blank" rel="noopener">https://youtu.be/TmDYxL7HV68?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=295</a><br>共同的特性</p>
<ol>
<li>column 是orthogonal</li>
<li>column 和是1</li>
</ol>
<p>orthogonal matrix它的column是orthonormal basis<br><a href="https://youtu.be/TmDYxL7HV68?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=644" target="_blank" rel="noopener">https://youtu.be/TmDYxL7HV68?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=644</a> </p>
<p>norm-preserving   &lt;===&gt;    orthogonal matrix</p>
<p>和orthogonal matrix相关的5个性质<br><a href="https://youtu.be/TmDYxL7HV68?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1027" target="_blank" rel="noopener">https://youtu.be/TmDYxL7HV68?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1027</a></p>
<p>如何判断一个matrix是不是orthogonal的？<br>看A的inverse 和A 的transpose是不是一样的？！</p>
<p><a href="https://youtu.be/TmDYxL7HV68?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1937" target="_blank" rel="noopener">https://youtu.be/TmDYxL7HV68?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1937</a></p>
<p>哈哈:TT就是掉眼泪的意思，掉过眼泪就什么事情没有发生了<br><a href="https://youtu.be/TmDYxL7HV68?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2031" target="_blank" rel="noopener">https://youtu.be/TmDYxL7HV68?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2031</a></p>
<p>Symmetric Matrices<br><a href="https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=6" target="_blank" rel="noopener">https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=6</a></p>
<ol>
<li><p>特点，它的eigenvalue一定都是实数</p>
</li>
<li><p>u和v一定是orthogonal</p>
</li>
<li><p>如果一个矩阵是symmetric的，那么它一定可以写成  P^T A P = D<br>P is orthogonal matrix<br>D is diagonal matrix<br><a href="https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1105" target="_blank" rel="noopener">https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1105</a></p>
</li>
</ol>
<p>为什么对于orthogonal matrix来说，P^T = P^(-1)<br>这个有直观的理解吗？<br><a href="https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1205" target="_blank" rel="noopener">https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1205</a></p>
<p>针对symmetric matrix来说，它一定是可以diagonalization的</p>
<p>这件事情有什么重要的呢？****<br>证明你找的的coordinate system是orthogonal basis<br><a href="https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1705" target="_blank" rel="noopener">https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1705</a></p>
<p>A = PDP^T<br>矩阵分解，分解出来的矩阵的rank都是1；<br><a href="https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1860" target="_blank" rel="noopener">https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1860</a></p>
<p>假设一个matrix是对称的symmetric的，那这个对称的matrix可以拆解乘成n个rank为1的matrix的weight sum,这些weight是eigenvalue<br><a href="https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2137" target="_blank" rel="noopener">https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2137</a></p>
<p>这个分解得到的matrix有一些性质<br><a href="https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2296" target="_blank" rel="noopener">https://youtu.be/0ijUQ-RfN3I?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2296</a></p>
<p>下一个主题Beyond Vector<br>Vector 更general的形态<br>一个function也是一个vector，<br><a href="https://youtu.be/o4dPfMkz_lw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=52" target="_blank" rel="noopener">https://youtu.be/o4dPfMkz_lw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=52</a></p>
<ol>
<li>一个matrix是一个vector</li>
<li>linear transform</li>
<li>polynomial</li>
<li>e^t也是一个vector  <a href="https://youtu.be/o4dPfMkz_lw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=300" target="_blank" rel="noopener">https://youtu.be/o4dPfMkz_lw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=300</a></li>
</ol>
<p>if a set of objects is vetor space, then the objects are vectors</p>
<p><a href="https://youtu.be/o4dPfMkz_lw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=411" target="_blank" rel="noopener">https://youtu.be/o4dPfMkz_lw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=411</a></p>
<p>需要满足8个条件，你才能构成一个vector space</p>
<p>review:subspace</p>
<p>linear transformation<br><a href="https://youtu.be/o4dPfMkz_lw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1695" target="_blank" rel="noopener">https://youtu.be/o4dPfMkz_lw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1695</a></p>
<p>isomorphism(同构)同样的架构 </p>
<p>basis（general）<br>——-&gt;</p>
<h3 id="general-space中也有basis"><a href="#general-space中也有basis" class="headerlink" title="general space中也有basis;"></a>general space中也有basis;</h3><p>我的想法：对于FFT来说，它的basis是不是cos和sin函数呢？<br><a href="https://youtu.be/o4dPfMkz_lw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2934" target="_blank" rel="noopener">https://youtu.be/o4dPfMkz_lw?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2934</a></p>
<p><a href="https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=168" target="_blank" rel="noopener">https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=168</a><br>同一个linear system 在不同的coordinate system 中是不一样的<br>微分，背后对应的matrix，长什么样子呢？<br><a href="https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=205" target="_blank" rel="noopener">https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=205</a></p>
<p>general eigenValue、eigenvector<br><a href="https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1180" target="_blank" rel="noopener">https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1180</a></p>
<p>transpose operator长什么样子？<br><a href="https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1324" target="_blank" rel="noopener">https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1324</a><br>找一下transpose的eigenvalue<br><a href="https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1502" target="_blank" rel="noopener">https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1502</a></p>
<h3 id="接下来要讲的是inner-product"><a href="#接下来要讲的是inner-product" class="headerlink" title="接下来要讲的是inner product"></a>接下来要讲的是inner product</h3><p><a href="https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1689" target="_blank" rel="noopener">https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=1689</a><br>inner product是dot product更general的形式<br>Dot product是inner product的special case</p>
<p>inner product of matrix<br>    Frobenius inner product<br>    <a href="https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2012" target="_blank" rel="noopener">https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2012</a></p>
<p>有了inner product就可以定义norm（长度）</p>
<p>inner product也可定义在function上边<br><a href="https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2211" target="_blank" rel="noopener">https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2211</a></p>
<p>定义orthogonal/orthogonormal basis有什么好处呢？<br>orthogonormal 的norm是1；<br><a href="https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2659" target="_blank" rel="noopener">https://youtu.be/7E7ZzTJFeng?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=2659</a></p>
<p>Singular Value Decomposition<br><a href="https://youtu.be/OEJ0wxxLO7M?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=24" target="_blank" rel="noopener">https://youtu.be/OEJ0wxxLO7M?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=24</a><br>和diagonalization作比较：<br>并不是所有的matrix都能做diagonalization，<br>但是，SVD就很神奇，它可以用在任何的matrix上：<br><a href="https://youtu.be/OEJ0wxxLO7M?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=67" target="_blank" rel="noopener">https://youtu.be/OEJ0wxxLO7M?list=PLJV_el3uVTsNmr39gwbyV-0KjULUsN7fW&amp;t=67</a></p>
<p>任何mn = mm * mn * nn<br>其中mm是matrix—&gt; U  —-&gt;column是independent的<br>其中mn(右边的)是matrix—&gt; 大sigma   是diagonal的，对角线是非负的，左上到右下是逐渐变小的<br>其中nn是matrix—&gt; V^T   row 是independent的</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Feng Zhiheng
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://fengzhiheng.github.io/2020/02/06/LinearAlgebra%E7%AC%94%E8%AE%B0/" title="LinearAlgebra笔记">https://fengzhiheng.github.io/2020/02/06/LinearAlgebra笔记/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%AA%E6%95%B4%E7%90%86/" rel="tag"># 未整理</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/04/%E5%88%9A%E5%BC%80%E5%A7%8B-%E7%9B%B4%E5%88%B0/" rel="prev" title="刚开始-直到">
      <i class="fa fa-chevron-left"></i> 刚开始-直到
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/02/07/%E7%BB%9D%E5%AD%A6/" rel="next" title="绝学">
      绝学 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  
  <div class="comments">
  <script src="https://utteranc.es/client.js" repo="FengZhiheng/FengZhiheng.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async></script>
  </div>
  
  

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性代数这门课讨论的是什么？"><span class="nav-number">1.</span> <span class="nav-text">线性代数这门课讨论的是什么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是线性系统？"><span class="nav-number">2.</span> <span class="nav-text">什么是线性系统？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性系统会关心什么问题？"><span class="nav-number">3.</span> <span class="nav-text">线性系统会关心什么问题？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性代数中基本概念？"><span class="nav-number">4.</span> <span class="nav-text">线性代数中基本概念？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何理解矩阵和向量相乘"><span class="nav-number">5.</span> <span class="nav-text">如何理解矩阵和向量相乘</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是Span"><span class="nav-number">6.</span> <span class="nav-text">什么是Span</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#有没有解的陈述，可是换一种说法"><span class="nav-number">7.</span> <span class="nav-text">有没有解的陈述，可是换一种说法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dependent和有解没有解是没有关系的。"><span class="nav-number">8.</span> <span class="nav-text">dependent和有解没有解是没有关系的。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是homogeneous"><span class="nav-number">9.</span> <span class="nav-text">什么是homogeneous?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ax-0有非零的解，那么A就是dependent，为什么？因为dependent就是这样定义的呢。"><span class="nav-number">10.</span> <span class="nav-text">Ax &#x3D; 0有非零的解，那么A就是dependent，为什么？因为dependent就是这样定义的呢。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在有解的前提下，有多少个解呢？"><span class="nav-number">11.</span> <span class="nav-text">在有解的前提下，有多少个解呢？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是一个Matrix的Rank呢？"><span class="nav-number">12.</span> <span class="nav-text">什么是一个Matrix的Rank呢？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#equivalent（等价的）什么是等价。"><span class="nav-number">13.</span> <span class="nav-text">equivalent（等价的）什么是等价。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#augmented-matrix增广矩阵"><span class="nav-number">14.</span> <span class="nav-text">augmented matrix增广矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是elementary-row-operation-（行变换？）"><span class="nav-number">15.</span> <span class="nav-text">什么是elementary row operation?（行变换？）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reduced-row-echelon-form-RREF-简化行梯形形式"><span class="nav-number">16.</span> <span class="nav-text">reduced row echelon form(RREF)(简化行梯形形式)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是row-echelon-form（要满足两个条件）"><span class="nav-number">17.</span> <span class="nav-text">什么是row echelon form（要满足两个条件）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是Reduced-row-echelon-form（要满足三个条件）"><span class="nav-number">18.</span> <span class="nav-text">什么是Reduced row echelon form（要满足三个条件）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#接下来要补充的一个概念是Pivot-column"><span class="nav-number">19.</span> <span class="nav-text">接下来要补充的一个概念是Pivot column</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reduced-row-echelon-form-和-linear-combination的关系"><span class="nav-number">20.</span> <span class="nav-text">Reduced row echelon form 和 linear combination的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reduced-row-echelon-form-和-Independent的关系"><span class="nav-number">21.</span> <span class="nav-text">reduced row echelon form 和 Independent的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reduced-row-echelon-form-和-Rank"><span class="nav-number">22.</span> <span class="nav-text">reduced row echelon form 和 Rank</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reduced-row-echelon-form-和-Span"><span class="nav-number">23.</span> <span class="nav-text">reduced row echelon form 和 Span</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何理解矩阵相乘，（这样记，不会忘记）"><span class="nav-number">24.</span> <span class="nav-text">如何理解矩阵相乘，（这样记，不会忘记）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AC-T-？-A乘上C的转置是什么"><span class="nav-number">25.</span> <span class="nav-text">(AC)^T &#x3D; ？ (A乘上C的转置是什么)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一些特殊的matrix"><span class="nav-number">26.</span> <span class="nav-text">一些特殊的matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是矩阵的逆-what’s-the-inverse-of-matrix"><span class="nav-number">27.</span> <span class="nav-text">什么是矩阵的逆(what’s the inverse of matrix)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#这一引入新的概念，singular，和non-singular。"><span class="nav-number">28.</span> <span class="nav-text">这一引入新的概念，singular，和non-singular。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一个矩阵是不是可逆的，10种等价说法，"><span class="nav-number">29.</span> <span class="nav-text">一个矩阵是不是可逆的，10种等价说法，</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Subspace"><span class="nav-number">30.</span> <span class="nav-text">Subspace</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Null-Space：所有Ax-0-的solution所构成的集合就是Null-A"><span class="nav-number">31.</span> <span class="nav-text">Null Space：所有Ax &#x3D; 0 的solution所构成的集合就是Null A</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Column-Space"><span class="nav-number">32.</span> <span class="nav-text">Column Space</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Basis（基础、支撑）"><span class="nav-number">33.</span> <span class="nav-text">Basis（基础、支撑）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#下来讲basis的重要的三个定理"><span class="nav-number">34.</span> <span class="nav-text">下来讲basis的重要的三个定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#怎么在两个坐标系中，做任意的转换？"><span class="nav-number">35.</span> <span class="nav-text">怎么在两个坐标系中，做任意的转换？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#这里引入一个新的概念similar；"><span class="nav-number">36.</span> <span class="nav-text">这里引入一个新的概念similar；</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从现在起，就进入eigenXXX的世界"><span class="nav-number">37.</span> <span class="nav-text">从现在起，就进入eigenXXX的世界</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#并不是所有的matrix都有eigenvector"><span class="nav-number">38.</span> <span class="nav-text">并不是所有的matrix都有eigenvector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#同一个eigenvalue会对应到很多的eigenvector"><span class="nav-number">39.</span> <span class="nav-text">同一个eigenvalue会对应到很多的eigenvector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#eigenspace-就是-A-lamdaI-n-的null-space"><span class="nav-number">40.</span> <span class="nav-text">eigenspace 就是(A-lamdaI_n)的null space</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#characteristic的特性"><span class="nav-number">41.</span> <span class="nav-text">characteristic的特性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是similar"><span class="nav-number">42.</span> <span class="nav-text">什么是similar?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Diagonalizable和eigenvalue、eigenvector有关系"><span class="nav-number">43.</span> <span class="nav-text">Diagonalizable和eigenvalue、eigenvector有关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何找到一个matrix的eigenvector和eigenvalue"><span class="nav-number">44.</span> <span class="nav-text">如何找到一个matrix的eigenvector和eigenvalue?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对于一个matrix先找到它的eigenvalue-然后就能找到这个eigenvalue对应eigenspace。eigenspace除了-0-之外，都是eigenvector"><span class="nav-number">45.</span> <span class="nav-text">对于一个matrix先找到它的eigenvalue,然后就能找到这个eigenvalue对应eigenspace。eigenspace除了{0}之外，都是eigenvector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对角化有什么应用？"><span class="nav-number">46.</span> <span class="nav-text">对角化有什么应用？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何知道一个矩阵有没有办法被对角化"><span class="nav-number">47.</span> <span class="nav-text">如何知道一个矩阵有没有办法被对角化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#orthogonal-projection-到底有什么用呢？"><span class="nav-number">48.</span> <span class="nav-text">orthogonal projection 到底有什么用呢？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最小而成近似-least-square-approximation"><span class="nav-number">49.</span> <span class="nav-text">最小而成近似 least square approximation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Orthogonal-basis（今天讲的是）"><span class="nav-number">50.</span> <span class="nav-text">Orthogonal basis（今天讲的是）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何把一组independent-vector-变成orthogonal-basis呢？"><span class="nav-number">51.</span> <span class="nav-text">如何把一组independent vector 变成orthogonal basis呢？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#general-space中也有basis"><span class="nav-number">52.</span> <span class="nav-text">general space中也有basis;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#接下来要讲的是inner-product"><span class="nav-number">53.</span> <span class="nav-text">接下来要讲的是inner product</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Feng Zhiheng"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Feng Zhiheng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">79</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Feng Zhiheng</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">147k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">4:05</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script color='0,0,255' opacity='0.5' zIndex='-5' count='5' src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-nest@latest/canvas-nest-nomobile.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
